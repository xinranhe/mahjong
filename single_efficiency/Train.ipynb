{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/xinran.he/GitProjects/mahjong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mahjong.shanten import Shanten\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "SHANTEN = Shanten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from single_efficiency.model import Model\n",
    "from single_efficiency import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    # Model params\n",
    "    \"initializer_gain\": 1.0,  # Used in trainable variable initialization.\n",
    "    \"hidden_size\": 16, # Model dimension in the hidden layers, input embedding dimension\n",
    "    \"num_hidden_layers\": 2, # Number of layers in the encoder stacks.\n",
    "    \"num_heads\": 2,\n",
    "    \"filter_size\": 16,\n",
    "    \n",
    "    # reward\n",
    "    \"gamma\": 0.9,\n",
    "    \n",
    "    # Dropout values (only used when training)\n",
    "    \"layer_postprocess_dropout\": 0.1,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"relu_dropout\": 0.1,\n",
    "    \n",
    "    # Params for transformer TPU\n",
    "    \"allow_ffn_pad\": True,\n",
    "    \n",
    "    # training\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"optimizer_adam_beta1\": 0.9,\n",
    "    \"optimizer_adam_beta2\": 0.997,\n",
    "    \"optimizer_adam_epsilon\": 1e-09,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLStrategy(object):\n",
    "    def __init__(self, sess, model, epsilon, is_debug=False):\n",
    "        self.epsilon = epsilon\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        self.is_debug = is_debug\n",
    "    \n",
    "    def discard(self, tiles):\n",
    "        hands = utils.tiles34_to_list(tiles)\n",
    "        if np.random.uniform() <= self.epsilon:\n",
    "            return random.choice(hands)\n",
    "        else:\n",
    "            predicts = model.inference(sess, hands)\n",
    "            if self.is_debug:\n",
    "                for t, v in zip(hands, predicts):\n",
    "                    print \"%s:%.3f\" % (utils.TO_GRAPH_LIST[t], v)\n",
    "            idx = np.argmax(predicts)\n",
    "            return hands[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = utils.load_hand([\"/Users/xinran.he/GitProjects/mahjong/data/single_hand_efficiency/20180102.txt\"])\n",
    "print len(hands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ROUND = 60\n",
    "MEMORY_SIZE = 1000000\n",
    "NEGATIVE_SAMPLE_RATE = 0.2\n",
    "NUM_STEP_EPOCH = 10000\n",
    "\n",
    "INIT_EPSILON = 0.5\n",
    "EPSILON_DECAY_PER_EPOCH = 0.9\n",
    "\n",
    "NEGATIVE_SAMPLE_RATE = 0.2\n",
    "MIN_NUM_INSTANCES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = []\n",
    "memory_pos = 0\n",
    "epsilon = INIT_EPSILON\n",
    "\n",
    "g=tf.Graph()\n",
    "with g.as_default():\n",
    "    sess = tf.Session()\n",
    "        \n",
    "    # initialize model\n",
    "    model = Model(PARAMS)\n",
    "    model.init(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "with g.as_default():\n",
    "    for epoch in xrange(NUM_EPOCHS):\n",
    "        print \"EPOCH: %d\" % epoch\n",
    "        # update target network\n",
    "        model.update_target_network(sess)\n",
    "        strategy = RLStrategy(sess, model, epsilon)\n",
    "\n",
    "        for step in xrange(NUM_STEP_EPOCH):\n",
    "            # random sample hand\n",
    "            init_hand = random.choice(hands)[1]\n",
    "            current_hand = [0] * utils.NUM_HAIS\n",
    "            left_tiles = [4] * utils.NUM_HAIS\n",
    "            for hai in init_hand:\n",
    "                left_tiles[hai] -= 1\n",
    "                current_hand[hai] += 1\n",
    "            yama = utils.tiles34_to_list(left_tiles)\n",
    "            random.shuffle(yama)\n",
    "\n",
    "            for i in xrange(MAX_ROUND):\n",
    "                state = utils.tiles34_to_list(current_hand)\n",
    "                shanten = int(SHANTEN.calculate_shanten(current_hand))\n",
    "\n",
    "                # call epsilon-greedy startegy to find action\n",
    "                discard = strategy.discard(current_hand)\n",
    "                discard_index = state.index(discard)\n",
    "\n",
    "                # draw a new tile\n",
    "                current_hand[discard] -= 1\n",
    "                new_tile = yama[i]\n",
    "                current_hand[new_tile] += 1\n",
    "\n",
    "                state_plus = utils.tiles34_to_list(current_hand)\n",
    "                shanten_plus = int(SHANTEN.calculate_shanten(current_hand))\n",
    "\n",
    "                if utils.is_agari(current_hand):\n",
    "                    is_terminal = 1\n",
    "                    reward = utils.get_total_score(current_hand, new_tile) / 100.0\n",
    "                else:\n",
    "                    is_terminal = 0\n",
    "                    reward = shanten - shanten_plus\n",
    "\n",
    "                if reward > 0 or np.random.uniform() <= NEGATIVE_SAMPLE_RATE:\n",
    "                    if len(memory) < MEMORY_SIZE:\n",
    "                        memory.append((state, state_plus, reward, discard_index, is_terminal, shanten, shanten_plus))\n",
    "                    else:\n",
    "                        memory[memory_pos] = (state, state_plus, reward, discard_index, is_terminal, shanten, shanten_plus)\n",
    "                        memory_pos = (memory_pos + 1) % MEMORY_SIZE\n",
    "\n",
    "                # agari is end of episode\n",
    "                if is_terminal > 0:\n",
    "                    break\n",
    "\n",
    "            if len(memory) >= MIN_NUM_INSTANCES:\n",
    "                # sample one batch from replay memory\n",
    "                batch = random.sample(memory, PARAMS[\"batch_size\"])\n",
    "                state_batch = np.array([b[0] for b in batch], dtype=np.int32)\n",
    "                state_plus_batch = np.array([b[1] for b in batch], dtype=np.int32)\n",
    "                reward_batch = np.array([b[2] for b in batch], dtype=np.float32)\n",
    "                action_batch = np.array([b[3] for b in batch], dtype=np.int32)\n",
    "                terminal_batch = np.array([b[4] for b in batch], dtype=np.int32)\n",
    "                loss = model.train(sess, (state_batch, state_plus_batch, reward_batch, action_batch, terminal_batch))\n",
    "                if step % 100 == 0:\n",
    "                    print \"step %d loss %.3f\" % (step, loss)\n",
    "            else:\n",
    "                if step % 100 == 0:\n",
    "                    print \"step %d memory size: %d\" % (step, len(memory))\n",
    "\n",
    "        # update epsilon\n",
    "        epsilon = epsilon * EPSILON_DECAY_PER_EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_strategy(sess, model, hands):\n",
    "    Qvalues = np.zeros(len(hands))\n",
    "    rewards = np.zeros(len(hands))\n",
    "    strategy = RLStrategy(sess, model, 0.01)\n",
    "    print \"Eval:\"\n",
    "    for i, hand in enumerate(hands):\n",
    "        if i % 50 == 0:\n",
    "            print i,\n",
    "        Qvalues[i] = np.max(model.inference(sess, hand))\n",
    "        \n",
    "        current_hand = [0] * utils.NUM_HAIS\n",
    "        left_tiles = [4] * utils.NUM_HAIS\n",
    "        for hai in hand:\n",
    "            left_tiles[hai] -= 1\n",
    "            current_hand[hai] += 1\n",
    "        yama = utils.tiles34_to_list(left_tiles)\n",
    "        random.shuffle(yama)\n",
    "        discount = 1.0\n",
    "        for r in xrange(MAX_ROUND):\n",
    "            state = utils.tiles34_to_list(current_hand)\n",
    "            discard = strategy.discard(current_hand)\n",
    "            # discard and draw a new tile\n",
    "            current_hand[discard] -= 1\n",
    "            new_tile = yama[r]\n",
    "            current_hand[new_tile] += 1\n",
    "            \n",
    "            if utils.is_agari(current_hand):\n",
    "                rewards[i] = discount * utils.get_total_score(current_hand, new_tile) / 100.0\n",
    "    print \"...Done\"\n",
    "    return np.mean(Qvalues), np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(sess, hand):\n",
    "    predictions = model.inference(sess, hand)\n",
    "    for hid, prediction in zip(hand, predictions):\n",
    "        print utils.TO_GRAPH_LIST[hid] + \":\" + str(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prediction(sess, hands[158][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

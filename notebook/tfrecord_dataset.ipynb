{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"/Users/xinran.he/GitProjects/mahjong\")\n",
    "\n",
    "from model.embedding_layer import EmbeddingSharedWeights\n",
    "from model.input_ops import get_feature_seq_embedding\n",
    "from model.dataset import input_function\n",
    "from model.params import PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_DATA_PATTERN = \"/Users/xinran.he/GitProjects/mahjong/data/tfrecord/20180101.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    features, labels = input_function([TRAINING_DATA_PATTERN], True, PARAMETERS)()\n",
    "    \n",
    "    pos_emb_layer = EmbeddingSharedWeights(\"pos_emb\", 40, 32)\n",
    "    hai_emb_layer = EmbeddingSharedWeights(\"pos_emb\", 71, 32)\n",
    "    feature_emb = get_feature_seq_embedding(features)\n",
    "    pos_emb = pos_emb_layer(tf.sparse.to_dense(features[\"pos_seq\"]))\n",
    "    hai_emb = hai_emb_layer(tf.sparse.to_dense(features[\"hai_seq\"]))\n",
    "\n",
    "    sess.run(tf.initializers.global_variables())\n",
    "    results = sess.run([tf.sparse.to_dense(features[\"hai_seq\"]), feature_emb, pos_emb, hai_emb])\n",
    "    #f, l = sess.run([features, labels])\n",
    "    #c_emb = sess.run(context_embedding_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parse_spec():\n",
    "    parse_spec = {\n",
    "        \"hai_seq\": tf.VarLenFeature(tf.int64),\n",
    "        \"pos_seq\": tf.VarLenFeature(tf.int64),\n",
    "        \"feature_seq\": tf.VarLenFeature(tf.int64),\n",
    "        \"label\": tf.FixedLenFeature([14], tf.float32),\n",
    "    }\n",
    "    parse_spec[\"current_field\"] = tf.FixedLenFeature([1], tf.int64)\n",
    "    parse_spec[\"round\"] = tf.FixedLenFeature([1], tf.int64)\n",
    "    parse_spec[\"center_field\"] = tf.FixedLenFeature([1], tf.int64)\n",
    "    parse_spec[\"center_oya\"] = tf.FixedLenFeature([1], tf.int64)\n",
    "    for pid in xrange(3):\n",
    "        parse_spec[\"player%d_oya\" % pid] = tf.FixedLenFeature([1], tf.int64)\n",
    "        parse_spec[\"player%d_field\" % pid] = tf.FixedLenFeature([1], tf.int64)\n",
    "        parse_spec[\"player%d_riichi\" % pid] = tf.FixedLenFeature([1], tf.int64)\n",
    "        parse_spec[\"player%d_claim\" % pid] = tf.FixedLenFeature([1], tf.int64)\n",
    "        parse_spec[\"player%d_order\" % pid] = tf.FixedLenFeature([1], tf.int64)\n",
    "        parse_spec[\"player%d_score\" % pid] = tf.FixedLenFeature([1], tf.int64)\n",
    "    return parse_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _embedding_column(key, num_values, num_dim):\n",
    "    column = tf.feature_column.categorical_column_with_identity(key, num_values)\n",
    "    column = tf.feature_column.embedding_column(column, num_dim)\n",
    "    return column\n",
    "\n",
    "def _numerical_column(key):\n",
    "    return tf.feature_column.numeric_column(key)\n",
    "\n",
    "def get_feature_columns():\n",
    "    columns = {}\n",
    "    # numerical columns\n",
    "    columns[\"center_oya\"] = _numerical_column(\"center_oya\")\n",
    "    for pid in xrange(3):\n",
    "        columns[\"player%d_oya\" % pid] = _numerical_column(\"player%d_oya\" % pid)\n",
    "        columns[\"player%d_riichi\" % pid] = _numerical_column(\"player%d_riichi\" % pid)\n",
    "    \n",
    "    # individual embedding columes\n",
    "    columns[\"round\"] = _embedding_column(\"round\", 4, 2)\n",
    "    for pid in xrange(3):\n",
    "        columns[\"player%d_claim\" % pid] = _embedding_column(\"player%d_claim\" % pid, 4, 2)\n",
    "        columns[\"player%d_order\" % pid] = _embedding_column(\"player%d_order\" % pid, 7, 4)\n",
    "        columns[\"player%d_score\" % pid] = _embedding_column(\"player%d_score\" % pid, 161, 15)\n",
    "        \n",
    "    # shared_embedding column\n",
    "    input_columns = []\n",
    "    input_columns.append(tf.feature_column.categorical_column_with_identity(\"current_field\", 4))\n",
    "    input_columns.append(tf.feature_column.categorical_column_with_identity(\"center_field\", 4))\n",
    "    for pid in xrange(3):\n",
    "        input_columns.append(tf.feature_column.categorical_column_with_identity(\"player%d_field\" % pid, 4))\n",
    "    shared_embedding_columns = tf.feature_column.shared_embedding_columns(input_columns, 2)\n",
    "    columns[\"current_field\"] = shared_embedding_columns[0]\n",
    "    columns[\"center_field\"] = shared_embedding_columns[1]\n",
    "    for pid in xrange(3):\n",
    "        columns[\"player%d_field\" % pid] = shared_embedding_columns[pid + 2]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _tfrecord_parse_fn(example_proto):\n",
    "    parsed_features = tf.parse_single_example(example_proto, get_parse_spec())\n",
    "    return parsed_features, parsed_features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context_embedding_mtx(parsed_features):\n",
    "    \"\"\"\n",
    "    Takes input features and returns the player context features\n",
    "    embedding lookup matrix.\n",
    "    \n",
    "    returns: context_embedding_mtx of size (batch, 4, embedding_dim (32)) \n",
    "    \"\"\"\n",
    "    all_columns = get_feature_columns()\n",
    "    features = []\n",
    "    for pid in xrange(3):\n",
    "        columns = [\n",
    "            all_columns[\"center_oya\"],\n",
    "            all_columns[\"round\"],\n",
    "            all_columns[\"current_field\"],\n",
    "            all_columns[\"center_field\"],\n",
    "            all_columns[\"player%d_oya\" % pid],\n",
    "            all_columns[\"player%d_riichi\" % pid],\n",
    "            all_columns[\"player%d_claim\" % pid],\n",
    "            all_columns[\"player%d_order\" % pid],\n",
    "            all_columns[\"player%d_score\" % pid],\n",
    "            all_columns[\"player%d_field\" % pid],\n",
    "        ]\n",
    "        features.append(tf.feature_column.input_layer(parsed_features, columns))\n",
    "    context_embedding_mtx = tf.stack([\n",
    "        tf.zeros_like(features[0], dtype=tf.float32),\n",
    "        features[0],\n",
    "        features[1],\n",
    "        features[2]\n",
    "    ])\n",
    "    context_embedding_mtx = tf.transpose(context_embedding_mtx, perm=[1, 0, 2])\n",
    "    context_embedding_mtx = tf.reshape(context_embedding_mtx, [-1, tf.shape(context_embedding_mtx)[2]])\n",
    "    return context_embedding_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_seq_embedding(features):\n",
    "    context_embedding_mtx = get_context_embedding_mtx(features)\n",
    "    feature_seq = features[\"feature_seq\"]\n",
    "    dense_feature_seq = tf.sparse.to_dense(feature_seq)\n",
    "    multiplier = tf.range(0, feature_seq.dense_shape[0]) * 4\n",
    "    mask = tf.to_int64(tf.not_equal(dense_feature_seq, 0))\n",
    "    feature_seq = dense_feature_seq + mask * tf.expand_dims(multiplier, axis=1)\n",
    "    context_seq_embedding = tf.nn.embedding_lookup(context_embedding_mtx, feature_seq)\n",
    "    return context_seq_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_DATA_PATTERN = \"/Users/xinran.he/GitProjects/mahjong/data/tfrecord/20180101.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    dataset = tf.data.TFRecordDataset(TRAINING_DATA_PATTERN, compression_type=\"GZIP\")\n",
    "    dataset = dataset.map(_tfrecord_parse_fn)\n",
    "    dataset = dataset.batch(2)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()    \n",
    "    \n",
    "    pos_emb_layer = EmbeddingSharedWeights(\"pos_emb\", 40, 32)\n",
    "    hai_emb_layer = EmbeddingSharedWeights(\"pos_emb\", 71, 32)\n",
    "    feature_emb = get_feature_seq_embedding(features)\n",
    "    pos_emb = pos_emb_layer(tf.sparse.to_dense(features[\"pos_seq\"]))\n",
    "    hai_emb = hai_emb_layer(tf.sparse.to_dense(features[\"hai_seq\"]))\n",
    "\n",
    "    sess.run(tf.initializers.global_variables())\n",
    "    results = sess.run([tf.sparse.to_dense(features[\"hai_seq\"]), feature_emb, pos_emb, hai_emb])\n",
    "    #f, l = sess.run([features, labels])\n",
    "    #c_emb = sess.run(context_embedding_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hais = results[3]\n",
    "print results[0]\n",
    "print hais[0, 10, :]\n",
    "print hais[0, 11, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print context_seq_embedding[4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print c_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print f[\"player0_oya\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print f[\"feature_seq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print f[\"pos_seq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
